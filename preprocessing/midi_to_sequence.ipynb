{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.int = int\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pretty_midi\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c8b2c",
   "metadata": {},
   "source": [
    "### Cheking Duplicate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1660a012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique MIDI files: 1630\n",
      "Total duplicates found: 0\n",
      "\n",
      "No duplicates found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# List of target composers\n",
    "target_composers = ['Bach', 'Beethoven', 'Chopin', 'Mozart']\n",
    "dataset_folder = Path('../data/midi/archive/midiclassics')\n",
    "\n",
    "dataset = []         # [(file_path, composer)]\n",
    "seen_filenames = {}  # filename -> path of first occurrence\n",
    "duplicates = []      # [(filename, duplicate_path, original_path)]\n",
    "\n",
    "for composer in target_composers:\n",
    "    composer_folder = dataset_folder / composer\n",
    "\n",
    "    for file_path in composer_folder.rglob('*'):\n",
    "        if file_path.suffix.lower() in ['.mid', '.midi']:\n",
    "            fname = file_path.name\n",
    "\n",
    "            if fname not in seen_filenames:\n",
    "                seen_filenames[fname] = str(file_path)\n",
    "                dataset.append((str(file_path), composer))\n",
    "            else:\n",
    "                duplicates.append((fname, str(file_path), seen_filenames[fname]))\n",
    "\n",
    "# Summary\n",
    "print(f\"Total unique MIDI files: {len(dataset)}\")\n",
    "print(f\"Total duplicates found: {len(duplicates)}\\n\")\n",
    "\n",
    "if duplicates:\n",
    "    print(\"Duplicate files found (filename, duplicate path, original path):\\n\")\n",
    "    for fname, dup_path, orig_path in duplicates:\n",
    "        print(f\"{fname}\\n  DUP: {dup_path}\\n  ORIG: {orig_path}\\n\")\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfc0fc0",
   "metadata": {},
   "source": [
    "### MIDI Parsing, Cleaning, and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45cd05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibility for older NumPy versions\n",
    "if not hasattr(np, 'int'):\n",
    "    np.int = int\n",
    "    \n",
    "target_composers = ['Bach', 'Beethoven', 'Chopin', 'Mozart']\n",
    "\n",
    "# We’re making a function that takes in the file path of a MIDI file and gives us back a numeric array of features\n",
    "def midi_to_feature_array(midi_path):\n",
    "\n",
    "    try:\n",
    "        # using the pretty_midi library\n",
    "        pm = pretty_midi.PrettyMIDI(str(midi_path))  # pm will be an object containing all the instruments, notes, tempos, and other music data from that file(PrettyMIDI needs a string, not a path obj)\n",
    "\n",
    "        # Corrupted files, wrong format will be skipped\n",
    "    except Exception as e: \n",
    "        print(f\"Error reading {midi_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # making an empty list rows to hold all the note data we care about\n",
    "    rows = []\n",
    "    for inst in pm.instruments: #  we loop over every instrument in the MIDI file.\n",
    "\n",
    "        #--- preprocessing 1 ----\n",
    "        if inst.is_drum:\n",
    "            continue  # drop drums (we want pitched musical notes like C or D,..) \n",
    "        # --- feature extraction ----\n",
    "        for n in inst.notes:\n",
    "            # keep the raw fields we need (note start/end, pitch, velocity, instrument program)\n",
    "            rows.append([n.start, n.end, n.pitch, n.velocity, inst.program])\n",
    "                #region each notes contains\n",
    "                # n.start -> when the note starts (in seconds).\n",
    "                # n.end -> when the note ends (in seconds).\n",
    "                # n.pitch -> the MIDI pitch number (e.g., 60 = Middle C).\n",
    "                # n.velocity -> how hard the note is played (volume).\n",
    "                # inst.program -> the MIDI instrument number (0 = piano, 40 = violin, etc.).          \n",
    "                #endregion each notes contains\n",
    "\n",
    "    if not rows:\n",
    "        return None  # nothing useful in this file\n",
    "\n",
    "    # We convert the list of notes into a NumPy array\n",
    "    # sorted by x[0] -> start time (so earlier notes come first)\n",
    "    # then by x[2] -> pitch (so if two notes start at the same time, sort by pitch)\n",
    "    # This gives us a consistent order for all notes\n",
    "    notes = np.array(sorted(rows, key=lambda x: (x[0], x[2])), dtype=np.float32)\n",
    "\n",
    "    # unpack columns for readability\n",
    "    starts, ends, pitch, vel, prog = notes.T\n",
    "    dur = ends - starts  # note duration( We calculate how long each note lasts).\n",
    "\n",
    "    # --- preprocessing  ----\n",
    "    # filtering out very short notes (less than 0.0001 seconds).\n",
    "    # remove ultra-short/invalid notes (cleanup)\n",
    "    # MIDI data often contains ultra-short notes that come from bad noisy conversion from another format, or human input glitches.\n",
    "    # These “blips” don’t have real musical meaning and can add noise to our  features, making the model’s job harder.\n",
    "    keep = dur > 1e-4\n",
    "    if not np.any(keep):\n",
    "        return None\n",
    "    # Look at the duration of each note. If it’s long enough, keep that note’s other features (start, pitch, velocity, program) as well. If it’s too short, throw away the whole note row\n",
    "    starts, dur, pitch, vel, prog = starts[keep], dur[keep], pitch[keep], vel[keep], prog[keep]\n",
    "\n",
    "    \n",
    "    # --- feature engineering  ----\n",
    "    # turning the cleaned data into model-friendly numbers\n",
    "    \n",
    "    # time gap between this note and the previous note\n",
    "    dt = np.diff(starts, prepend=starts[0])\n",
    "    dt = np.log1p(dt)  # We take the logarithm of the gaps. Because musical timing can vary the can be tiny gaps vs. long pauses.  logs compress the range so big gaps don’t dominate.\n",
    "\n",
    "    #  pitch change between this note and the previous note.\n",
    "    interval = np.diff(pitch, prepend=pitch[0]) # Positive = up in pitch, negative = down\n",
    "    interval = np.clip(interval, -24, 24)  # We limit (clip) pitch changes to ±24 semitones (two octaves) because large jumps are rare, and extreme values can mess with training\n",
    "\n",
    "    # pc = pitch class -> the position of the note within an octave (0 = C, 1 = C#, … 11 = B).\n",
    "    # This ignores octave number and focuses on note's name.\n",
    "    pc = np.mod(pitch, 12)\n",
    "\n",
    "    # duration relative to time until next note (captures staccato/legato feel)\n",
    "    tnext = np.r_[starts[1:], starts[-1] + dur[-1]]           # tnext = when the next note starts, For the last note, we pretend the “next start” is at the note’s end.\n",
    "    time_to_next = np.clip(tnext - starts, 1e-3, None)        # time until the next note, We clip at 1e-3 so we don’t divide by zero later\n",
    "\n",
    "    # dur_ratio = dur\n",
    "    # dur_ratio = np.clip(dur / time_to_next, 0., 3.)  # how long a note lasts relative to the gap until the next not.  Near 1 = legato (notes connected). We clip to [0, 3] to avoid extreme values. \n",
    "    dur_ratio = np.log1p(dur / time_to_next) # Best result between these three feature was with this, not a significant but slightly better, betther than other experiements although it was a slight improvement\n",
    "              \n",
    "\n",
    "    # We stack all our engineered features into a single 2D array\n",
    "    # plus useful raw-ish signals (velocity, program)\n",
    "    #dt - > time gap to previous note\n",
    "    #interval -> pitch change\n",
    "    #pc -> pitch class\n",
    "    #dur_ratio -> duration relative to next note #\n",
    "    feats = np.stack([dt, interval, pc, dur_ratio, vel, prog], axis=1).astype(np.float32)\n",
    "    return feats\n",
    "\n",
    "    # Why we ignored pitch?\n",
    "    # later on we found out using pitch feature dropped the accuracy ( Test Accuracy: dropped from 0.8296 to 0.7996)\n",
    "    # THis 6‑feature set (pc‑based) was cleaner and simpler.\n",
    "    # The new file balanced_chunks_seq70_7features.pkl is not “worse code,” but it changes the learning problem—more noise/features without compensating changes (tuning, capacity, or normalization strategies).\n",
    "    \n",
    "\n",
    "\n",
    "# Look through a root folder to load every MIDI file for that composer to convert them into feature arrays using midi_to_feature_array\n",
    "def load_midi_dataset(root_dir, target_composers):\n",
    "    \n",
    "\n",
    "    data_dict = {} # Keys will be composer names and values will be lists of feature arrays (one per MIDI file for that composer).\n",
    "    root_dir = Path(root_dir) # Path gives us nice methods like .rglob() for finding files\n",
    "    for composer in target_composers: \n",
    "        composer_folder = root_dir / composer # making the full path to this composer’s folder.\n",
    "        if not composer_folder.exists():\n",
    "            print(f\"Warning: Folder not found for {composer}\")\n",
    "            continue\n",
    "\n",
    "        pieces = [] # empty list to hold all the processed pieces for this composer\n",
    "\n",
    "        # We search recursively inside this composer’s folder for files ending in .mid or .midi.\n",
    "        # .rglob(\"*.mid\") finds all .mid files in all subfolders.\n",
    "        midi_files = list(composer_folder.rglob(\"*.mid\")) + list(composer_folder.rglob(\"*.midi\"))\n",
    "\n",
    "        for file_path in tqdm(midi_files, desc=f\"Processing {composer}\"): #  wrap the loop with tqdm so we get a nice progress bar in the terminal.\n",
    "            arr = midi_to_feature_array(file_path)   # calling midi_to_feature_array to actually load it, clean it, and convert it into a numeric feature array for the model.\n",
    "            if arr is not None:\n",
    "                pieces.append(arr)\n",
    "\n",
    "        data_dict[composer] = pieces # storing the list of that composer’s pieces into our main dictionary under their name.\n",
    "        # at final we have 2D arrays looks like \n",
    "        # [ [dt, interval, pc, dur_ratio, vel, prog, pitch],   # note 1\n",
    "        # [dt, interval, pc, dur_ratio, vel, prog, pitch],   # note 2\n",
    "        # ...\n",
    "        # ]\n",
    "\n",
    "        print(f\"Loaded {len(pieces)} pieces for {composer}\")\n",
    "    return data_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1dc7b637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Bach: 100%|██████████| 1024/1024 [00:30<00:00, 33.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1024 pieces for Bach\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Beethoven:   4%|▍         | 9/213 [00:00<00:06, 31.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading ..\\data\\midi\\archive\\midiclassics\\Beethoven\\Anhang 14-3.mid: Could not decode key with 3 flats and mode 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Beethoven: 100%|██████████| 213/213 [00:34<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 212 pieces for Beethoven\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chopin: 100%|██████████| 136/136 [00:07<00:00, 19.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 136 pieces for Chopin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Mozart:  61%|██████    | 156/257 [00:15<00:06, 14.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading ..\\data\\midi\\archive\\midiclassics\\Mozart\\Piano Sonatas\\Nueva carpeta\\K281 Piano Sonata n03 3mov.mid: Could not decode key with 2 flats and mode 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Mozart: 100%|██████████| 257/257 [00:26<00:00,  9.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 256 pieces for Mozart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# the root directory of all our MIDI data\n",
    "dataset_folder = Path(\"../data/midi/archive/midiclassics\")\n",
    "# Calling the above preprocessing and feature engineering functions \n",
    "parsed_data = load_midi_dataset(dataset_folder, target_composers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9184196",
   "metadata": {},
   "source": [
    "### Transposition Augmentation, Sliding-Window Chunking (seq_len=70, stride=35), and Class Balancing (Downsample to Smallest Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "495a6de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset saved to ../models/RNN/balanced_chunks_seq70.pkl with 10152 chunks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# region Data Augmentation\n",
    "# without augmentation we faced this result in our evaluation and in confusion matrix with about 20% misunderstood with eachother in evaluation.\n",
    "#             precision recall    f1-score   support\n",
    "#Bach         0.82      0.90      0.86       525\n",
    "#Beethoven    0.64      0.59      0.62       525\n",
    "#Chopin       0.77      0.86      0.81       525\n",
    "#Mozart       0.67      0.58      0.62       525\n",
    "# Why?\n",
    "# We went for this function because Beethoven and Mozart had fewer or less varied training examples, and in the test results they were often misclassified compared to Bach or Chopin.\n",
    "# The model struggled to correctly identify Beethoven and Mozart pieces — likely because it didn’t see enough diverse examples during training.\n",
    "\n",
    "## chunks are arrays where column index 2 is the pitch class (values 0–11)\n",
    "def transpose_chunk(chunk, semitones):  # If we take a melody and shift all notes up or down by a few semitones, it’s still the same musical pattern, just in a different key.\n",
    "\n",
    "    out = chunk.copy() # work on a copy so we don’t accidentally modify the original input\n",
    "    out[:, 2] = np.mod(out[:, 2] + semitones, 12)  # augmentation only; keeps pattern, changes key- adding 'semitones' and wrap with mod 12 so it stays a valid pitch class. Ex: (class is 10(A#) + +4) mod 12 -> 2(D) \n",
    "    return out\n",
    "\n",
    "def maybe_augment_chunks(chunks, label,\n",
    "                         aug_per_chunk=1, #  we want to help Beethoven and Mozart not enough informative data a bit without exploding dataset size. If it goes in bad result we can add extra +2, +3 for example\n",
    "                         semis_choices=(-4, -2, 2, 4), # Why shifts = (-4, -2, 2, 4)?  Small transpositions keep phrases in a believable range. Big jumps push lines into outside typical instrument zones. Using both up and down shifts avoids biasing the dataset toward higher or lower keys.\n",
    "                         targets=(\"Beethoven\", \"Mozart\")): \n",
    "    \"\"\"\n",
    "    If the piece's label/composer is in `targets`, make extra versions of each chunk\n",
    "    by transposing pitch class up/down by a few semitones. \n",
    "    \"\"\"\n",
    "    # If this label isn’t in our target list (or augmentation count is 0), do nothing.\n",
    "    if label not in targets or aug_per_chunk <= 0:\n",
    "        return chunks\n",
    "    out = []\n",
    "    for ch in chunks:\n",
    "        out.append(ch)  # Always keep the original\n",
    "        for _ in range(aug_per_chunk):\n",
    "            s = random.choice(semis_choices) # randomly pick a shift like -4, -2, +2, +4 semitones\n",
    "            out.append(transpose_chunk(ch, s))  # add a transposed copy ( same shape, but new key)\n",
    "    return out\n",
    "\n",
    "\n",
    "# endregion Data Augmentation\n",
    "\n",
    "\n",
    "# We already parsed the MID files (load_midi_dataset)\n",
    "# We exatraced engineered features dt, interval, pc, dur_ratio, velocity, program\n",
    "# given a parsed_data dictionary in conclusion in the above cell where we have data as result like this: { \"Bach\": [piece1_array, piece2_array, …], \"Beethoven\": ....}\n",
    "\n",
    "# This functuin takes the parsed data and cuts it into fixed-length chunks \n",
    "def create_balanced_chunks(data_dict,\n",
    "                           seq_len=70, # 70 is actually grounded in both EDA evidence and model performance testing ( Bach has some very short pieces, 100: too long -> more padding for short pieces , 50: too short that chunks don’t carry enough musical structure)\n",
    "                           stride=35, # This creates overlap between chunks. First chunk: notes 0 -> 69, second chunk: 35 ->104, ... without overlapping we might miss patterns, with small stride, we have many chunk similar to each other and after playing with this we found it more fair\n",
    "                           min_real_notes=50, # with a sq=70 If a chunk has fewer than 50 real notes, it gets thrown away( zero-paddings for example) \n",
    "                           max_chunks_per_piece=20, # is a cap to stop any single music piece from giving too many chunks and dominating the dataset\n",
    "                           aug_per_chunk=1, # For each original chunk that qualifies for augmentation (Beethoven and Mozart in our case) create one extra augmented copy by shifting he pitch classes with below setting \n",
    "                           aug_semis=(-4, -2, 2, 4),\n",
    "                           aug_targets=(\"Beethoven\", \"Mozart\")):\n",
    "\n",
    "\n",
    "    all_chunks = defaultdict(list) # each new key automatically starts with an empty list {'Bach': ['chunk1', 'chunk2'], 'Mozart': ['chunkA'] # } with a normal chunks_by_composer = {} I faced error \"'Bach' key doesn't exist yet\"\n",
    "\n",
    "    # Loop over composers and their pieces (parsed_data dict)\n",
    "    for composer, pieces in data_dict.items():\n",
    "        for piece in pieces:\n",
    "            n_notes = len(piece) # number of rows (notes/events) in this piece\n",
    "            chunks_for_piece = [] # collecting this piece’s chunks before adding to the composer bucket\n",
    "\n",
    "            # PREPROCESS: handle short pieces by padding to seq_len\n",
    "            if n_notes < seq_len:\n",
    "                pad_len = seq_len - n_notes # how many rows we need to reach seq_len\n",
    "                padded = np.vstack([piece, np.zeros((pad_len, piece.shape[1]))]) # pad with zeros; zeros mean \"no note\" and are safe (can be masked later)\n",
    "                # only keep if the piece still has enough real notes to matter\n",
    "                if n_notes >= min_real_notes:\n",
    "                    chunks_for_piece.append(padded)\n",
    "\n",
    "            else:\n",
    "                # PREPROCESS: sliding window to make fixed-length chunks\n",
    "                #start positions:0, starting index of the chunk inside the piece\n",
    "                # how far we slide the window forward after each chunk (35 here)\n",
    "                for start in range(0, n_notes - seq_len + 1, stride): \n",
    "                    chunk = piece[start:start + seq_len] # This grabs seq_len rows from the piece. Each row is a note with all its features\n",
    "\n",
    "                    #Checking how many real notes (non-zero rows) are inside the chunk, at least min_real_notes (50 here).\n",
    "                    if np.count_nonzero(np.any(chunk != 0, axis=1)) >= min_real_notes:\n",
    "                        chunks_for_piece.append(chunk)\n",
    "\n",
    "            # if this composer is in the augmentation target list (Beethoven, Mozart that already we have) make extra versions of each chunk by shifting their pitch classes up or down\n",
    "            chunks_for_piece = maybe_augment_chunks(\n",
    "                chunks_for_piece, composer,\n",
    "                aug_per_chunk=aug_per_chunk,\n",
    "                semis_choices=aug_semis,\n",
    "                targets=aug_targets\n",
    "            )\n",
    "\n",
    "            # cap the number of chunks per piece\n",
    "            if len(chunks_for_piece) > max_chunks_per_piece:\n",
    "                chunks_for_piece = random.sample(chunks_for_piece, max_chunks_per_piece)\n",
    "\n",
    "            # collect for this composer\n",
    "            all_chunks[composer].extend(chunks_for_piece)\n",
    "\n",
    "    # handling imbalanced data: class balancing by downsampling to smallest class size among all composers\n",
    "    min_count = min(len(chunks) for chunks in all_chunks.values())\n",
    "    balanced_chunks = []\n",
    "    for composer, chunks in all_chunks.items():\n",
    "        sampled = random.sample(chunks, min_count)  # downsample to balance classes\n",
    "        balanced_chunks.extend([(chunk, composer) for chunk in sampled]) # Adding them all to the balanced_chunks list in (features, label) format\n",
    "\n",
    "    # Shuffling the final dataset so that the order of composers and chunks is mixed\n",
    "    random.shuffle(balanced_chunks) \n",
    "    return balanced_chunks\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# call to run the whole preprocessing + augmentation + balancing pipeline we just broke down\n",
    "balanced_dataset = create_balanced_chunks(\n",
    "    parsed_data, \n",
    "    seq_len=70, stride=35, min_real_notes=50, max_chunks_per_piece=20,\n",
    "    aug_per_chunk=1, aug_semis=(-4, -2, 2, 4), aug_targets=(\"Beethoven\", \"Mozart\")\n",
    ")\n",
    "\n",
    "\n",
    "# saving-to-disk step so we don’t have to re-run the whole preprocessing pipeline every time. \n",
    "pkl_name = \"../models/RNN/balanced_chunks_seq70.pkl\"\n",
    "with open(pkl_name, \"wb\") as f:\n",
    "    pickle.dump(balanced_dataset, f)\n",
    "print(f\"Balanced dataset saved to {pkl_name} with {len(balanced_dataset)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66791ac",
   "metadata": {},
   "source": [
    "### Feature Distribution Sanity Check (min/median/95th/max for dt, interval, pc, dur_ratio, velocity, program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b21e9b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt        min=0.0000 p50=0.0038 p95=0.3483 max=2.3840\n",
      "interval  min=-24.0000 p50=3.0000 p95=21.0000 max=24.0000\n",
      "pc        min=0.0000 p50=5.0000 p95=11.0000 max=11.0000\n",
      "dur_ratio min=0.0052 p50=3.8501 p95=6.7548 max=10.3669\n",
      "velocity  min=1.0000 p50=74.0000 p95=116.0000 max=127.0000\n",
      "program   min=0.0000 p50=0.0000 p95=68.0000 max=120.0000\n"
     ]
    }
   ],
   "source": [
    "# Peek at distributions (sanity check)\n",
    "with open(\"../data/note_sequences/balanced_chunks_seq70.pkl\", \"rb\") as f:\n",
    "    balanced_dataset = pickle.load(f)\n",
    "\n",
    "stacked = np.vstack([c for c,_ in balanced_dataset])  # \n",
    "names = [\"dt\",\"interval\",\"pc\",\"dur_ratio\",\"velocity\",\"program\"]\n",
    "for i,n in enumerate(names):\n",
    "    col = stacked[:,i]\n",
    "    col = col[np.isfinite(col)]\n",
    "    print(f\"{n:<9} min={col.min():.4f} p50={np.median(col):.4f} \"\n",
    "          f\"p95={np.percentile(col,95):.4f} max={col.max():.4f}\")\n",
    "# p50 = the median\n",
    "# p95 = the 95th percentile\n",
    "# If max ≫ p95, w’ve got outliers.\n",
    "# dt: time gap to the previous note (seconds, log1p-scaled). Smaller = faster notes; larger = pauses.\n",
    "# interval: pitch change from previous note (semitones, clipped [-24, 24]). + up, - down.\n",
    "# pc: pitch class 0–11 (C=0 … B=11). Octave ignored (note name only).\n",
    "# dur_ratio: better than raw duration and no-duration features, and avoids hard-cap saturation.\n",
    "# velocity: how hard the note is played (MIDI 1–127). Bigger = louder.\n",
    "# program: MIDI instrument program (0–127). 0 = Acoustic Grand Piano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fc4baa",
   "metadata": {},
   "source": [
    "### Chunk Count by Composer (post-balancing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "878e0eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bach: 2538 chunks\n",
      "Beethoven: 2538 chunks\n",
      "Chopin: 2538 chunks\n",
      "Mozart: 2538 chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Count the number of chunks per composer\n",
    "unique_labels, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f\"{label}: {count} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d6e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a511_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
